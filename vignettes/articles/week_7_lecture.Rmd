---
title: 'Forests of the Future: Week 7 Lecture'
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    self_contained: yes
description: Forests of the Future Week 7 Lecture
---

```{css echo = FALSE}
@media print {
  .topicsContainer,
  .topicActions,
  .exerciseActions .skip {
    display: none;
  }
  .topics .tutorialTitle,
  .topics .section.level2,
  .topics .section.level3:not(.hide) {
    display: block;
  }
  .topics {
    width: 100%;
  }
  .tutorial-exercise, .tutorial-question {
    page-break-inside: avoid;
  }
  .section.level3.done h3 {
    padding-left: 0;
    background-image: none;
  }
  .topics .showSkip .exerciseActions::before {
    content: "Topic not yet completed...";
    font-style: italic;
  }
}
```

```{r setup, include = FALSE}
library(fofpack)
library(tidyverse)
library(tidymodels)

options(datatable.alloccol = 1024,
        datatable.verbose = FALSE)

data("IRC")
data("RF_abund")
fish_dat <- RF_abund %>%
  filter(SpeciesName == "Thalassoma pavo") %>%
  mutate(Presence = as.factor(Presence))

```

## Model Tuning

- Going back to our reef fish example, we saw the we had some major overfitting issues with our boosted tree model.
- In this class we will see how to tune hyper-parameters of model to improve performance.

---


```{r RF2}
library(fofpack)
library(tidyverse)
library(tidymodels)

data("RF_abund")
fish_dat <- RF_abund %>%
  filter(SpeciesName == "Thalassoma pavo") %>%
  mutate(Presence = as.factor(Presence))
```

## Test Set

- Split off a test set to evaluate model at the very end!


```{r RF22}
set.seed(1234)
data_split <- initial_split(fish_dat, 0.8, strata = Presence)

train_data <- training(data_split)
test_data <- testing(data_split)

train_data
test_data
```

## Make a `recipe`

- Transform data to make it less skewed and centered on zero.


```{r recipe2}
RF_recipe <- recipe(train_data,
                    Presence ~ MeanTemp_CoralWatch + Depth_Site) %>%
  step_YeoJohnson(MeanTemp_CoralWatch, Depth_Site) %>%
  step_normalize(MeanTemp_CoralWatch, Depth_Site)
RF_recipe
```

## Make a `model`

- Setup the model object


```{r model1}
RF_mod <-
  boost_tree() %>%
  set_engine('xgboost') %>%
  set_mode('classification')

RF_mod

```

## A `workflow` is a `recipe` and a `model`

```{r wf1}
RF_wf <- workflow() %>%
  add_recipe(RF_recipe) %>%
  add_model(RF_mod)

RF_wf

```

## Use `fit` to run a workflow then visualize predictions

Run the following code:


```{r wf2}
RF_fit <- RF_wf %>%
  fit(train_data)

RF_train_preds <- augment(RF_fit, train_data)

ggplot(RF_train_preds, aes(MeanTemp_CoralWatch, as.numeric(Presence) - 1)) +
  geom_line(aes(y = .pred_1)) +
  geom_point() +
  theme_minimal()

```

## How good were the predictions on the test data?

```{r wf3}
augment(RF_fit, test_data) %>%
    roc_auc(.pred_0, truth = Presence)
```

## How to tune the model to improve performance

- We use `tune()` as a placemarker for a hyper-parameter


```{r model10}
RF_mod <-
  boost_tree(trees = tune(), learn_rate = tune()) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

RF_wf <- workflow() %>%
  add_recipe(RF_recipe) %>%
  add_model(RF_mod)

RF_wf

```

## Resamples for Cross Validation

```{r model11}
fish_folds <- vfold_cv(train_data, v = 6, repeats = 2, strata = Presence)
fish_folds
```

## Create Hyperparameter Grid

```{r grid}
tree_grid <- grid_regular(trees(range = c(0, 5), trans = scales::log_trans()),
                          learn_rate(),
                          levels = 5)
tree_grid
```

## Run the model on a grid of hyper-parameters

```{r model12}
RF_res <- RF_wf %>%
  tune_grid(fish_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

RF_res
```

  

## Best models?

We can see the best models using the `show_best()` function.


```{r model13}
RF_res %>%
  show_best()
```

---

We can plot all the results using `collect_metrics()` to collect our `roc_auc` values and then directly feed them into a `ggplot2` plot.


```{r model_plot}
RF_res %>%
  collect_metrics() %>%
  mutate(learn_rate = factor(learn_rate)) %>%
  ggplot(aes(trees, mean, color = learn_rate)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_color_viridis_d(begin = .95, end = 0) +
  scale_x_log10() +
  theme_minimal()
```

## Final Fit

```{r final_fit}
best_tree <- RF_res %>%
  select_best("roc_auc")

final_wf <- RF_wf %>% 
  finalize_workflow(best_tree)

final_wf
```

---


```{r final_fit2}
final_fit <- 
  final_wf %>%
  last_fit(data_split)

final_fit

final_fit %>%
  collect_metrics()

RF_final_preds <- augment(final_fit$.workflow[[1]],
                          fish_dat)

ggplot(RF_final_preds, aes(MeanTemp_CoralWatch, as.numeric(Presence) - 1)) +
  geom_line(aes(y = .pred_1)) +
  geom_point() +
  ylab("Probability of Presence") +
  theme_minimal()
```

## Plot Final Predictions on All Data

```{r final_fit3}
RF_final_preds <- augment(final_fit$.workflow[[1]],
                          fish_dat)

ggplot(RF_final_preds, aes(MeanTemp_CoralWatch, as.numeric(Presence) - 1)) +
  geom_line(aes(y = .pred_1)) +
  geom_point() +
  ylab("Probability of Presence") +
  theme_minimal()
```

## Endangered Pine Rocklands

- Pine Rocklands, as with an 'ecosystem' is defined by its unique assemblage of
species
- Can we predict which plant communities are Pine Rocklands from their species
assemblage?


## Flip the Script

- We model the distribution of Pine Rockland throughout South Florida parks
- Response is Pine Rockland, Yes or No?
- Predictors are: plant species
- Pine Rocklands is 'the species' being modelled, the presence or absence of 
different plants are the 'environmental variables'
- In the assignment this week we will all attempt this model bu using different
models and / or parameters


## Let's Go Through the Model Together

```{r PR_model1}
data("IRC")
data("parks_LC")

parks_LC
```

---

Parks that have Pine Rockland:
 

```{r PR_model1-2}
PR_parks <- parks_LC %>%
  filter(NAME_STATE == "Pine Rockland",
         prop > 0.05)

PR_parks

nrow(PR_parks) / n_distinct(parks_LC$area_name)
```

---

Turn this into a presence / absence column
 

```{r PR_model2}
PR_parks <- parks_LC %>%
  select(area_name) %>%
  distinct() %>%
  left_join(PR_parks %>%
              mutate(Pine_Rockland = "Present") %>%
              select(area_name, Pine_Rockland)) %>%
  mutate(Pine_Rockland = ifelse(is.na(Pine_Rockland), "Absent", Pine_Rockland) %>%
           as.factor())

PR_parks
```

---

Now for our predictors. Introducing `pivot_wider()` from `tidyr`
 

```{r PR_model3}
spec_preds <- IRC %>%
  mutate(Occurrence = ifelse(Occurrence == "Present", 1, 0)) %>%
  select(area_name, ScientificName, Occurrence) %>%
  distinct(area_name, ScientificName, .keep_all = TRUE) %>%
  pivot_wider(names_from = ScientificName,
              values_from = Occurrence)

spec_preds
```

---

Merge the data.
 

```{r PR_model4}
PR_parks <- PR_parks %>%
  left_join(spec_preds)

PR_parks

```

## Try a Model, Step by Step

Split the data:


```{r PR_model_split}

set.seed(1234)
PR_split <- initial_split(PR_parks, 0.8, strata = Pine_Rockland)

PR_train <- training(PR_split)
PR_test <- testing(PR_split)

```

---

Setup a recipe (very simple this time):


```{r PR_model_recipe}
PR_recipe <- recipe(Pine_Rockland ~ ., data = PR_train) %>%
  update_role(area_name, new_role = "id variable")

```

---

Setup a model:


```{r PR_model_mod}
PR_mod <-
  rand_forest(trees = tune(), mtry = tune()) %>%
  set_engine('ranger') %>%
  set_mode('classification')

PR_wf <- workflow() %>%
  add_recipe(PR_recipe) %>%
  add_model(PR_mod)

PR_wf

```

---

Setup resamples:


```{r PR_model_resample}
PR_folds <- vfold_cv(PR_train, v = 6, strata = Pine_Rockland)
PR_folds
```

---

Fit workflow on tuning grid:


```{r PR_model_tune}
PR_tune <- PR_wf %>%
  tune_grid(PR_folds,
            grid = 25,
            metrics = metric_set(roc_auc),
            control = control_grid(verbose = TRUE))

PR_tune
```

--- 

Find best model:


```{r PR_model_best}
PR_tune %>%
  show_best()

PR_best <- PR_tune %>%
  select_best("roc_auc")

PR_best
```

---

Do final fit:


```{r PR_final_fit}
PR_final_wf <- PR_wf %>% 
  update_model(rand_forest(trees = PR_best$trees,
                           mtry = PR_best$mtry) %>%
                 set_mode("classification") %>%
                 set_engine("ranger",
                            importance = "impurity"))

PR_final_fit <- 
  PR_final_wf %>%
  last_fit(PR_split)

PR_final_fit %>%
  collect_metrics()
```

---

Importance scores:


```{r PR_final_fit2}
library(vip)

PR_final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)

```

---


```{r PR_final_fit3}
PR_final_wf2 <- PR_wf %>% 
  update_model(rand_forest(trees = PR_best$trees,
                           mtry = PR_best$mtry) %>%
                 set_mode("classification") %>%
                 set_engine("ranger",
                            importance = "permutation"))

PR_final_wf2 %>%
  last_fit(PR_split) %>%
  extract_fit_parsnip() %>% 
  vip(num_features = 30)


```

